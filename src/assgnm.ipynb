{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc19852e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b474baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import openai\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pinecone\n",
    "import tiktoken\n",
    "\n",
    "from itertools import chain\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from Credentials import OPENAI_API_KEY, PINECONE_API_KEY\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone as lang_pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c764df",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d6ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "curr_dir = (os.getcwd()).replace('\\\\', '/')\n",
    "file_path = curr_dir + '/Dataset/Corrective RAG.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d425de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea4a3b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = tiktoken_len(text[0].page_content)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a97dfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(document):\n",
    "       \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pdf_text = loader.load()\n",
    "    pdf_text[0].page_content = pdf_text[0].page_content.replace('\\n', '')\n",
    "\n",
    "    return pdf_text    \n",
    "        \n",
    "def pdf_to_chunks(text, \n",
    "                  chunk_size, \n",
    "                  overlap):\n",
    "    \n",
    "    split_text = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                chunk_overlap=overlap, \n",
    "                                                length_function=tiktoken_len)\n",
    "    docs = split_text.split_text(text[0].page_content)\n",
    "    \n",
    "    chunks = [str(doc) for doc in docs]\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b753a09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction to RAG  Retrieval -augmented generation (RAG) is an AI framework for improving the quality of  LLM -generated  responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM -based question answering system has two main benefits: It ensures that the model has access to the  most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.  Prior research has introduced retrieval techniques to incorporate relevant knowledge and augment  generation, as  exemplified  by retrieval  augmented  generation ( RAG). In this framework, the input to  models is augmented  by prepending  relevant  documents  that are retrieved  from  an external  knowledge  corpus .  While  RAG  serves  as a practicable complement to  LLMs, its effectiveness is contingent upon the relevance  and accuracy  of the retrieved  documents . The heavy  reliance  of generation  on the retrieved  knowledge r aises significant concerns about the  model’s  behaviour  and performance in scenarios  where  retrieval  may  fail or return  inaccurate  results.  A low-quality  retriever  is prone  to introducing a substantial amount of irrelevant information  impeding the  models from  acquiring accurate knowledge  and potentially  misleading  them,  resulting  in issues  such  as hallucinations.  However,  most conventional  RAG  approaches  indiscriminately  incorporate  the retrieved documents,  regardless  of whether  these  documents are  relevant  or not. Furthermore, current  methods  mostly  treat  complete  documents as  reference  knowledge  both  during  retrieval  and utilization . But a considerable  portion  of the text within these retrieved  documents is often  nonessential for  generation,  which  should  not have been  equally  referred  to and involved  in RAG.  Introduction to CRAG  Although retrieval -augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retri eval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different k nowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large -scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose -then recompo se algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them.  Limitations  While we primarily proposed to improve the RAG framework from a corrective perspective, how to detect and correct the wrong knowledge more accurately and effectively still requires further study. Although CRAG can be seamlessly coupled with various RAG -based approaches, fine -tuning a retrieval evaluator is inevitable. In addition, potential bias introduced by web searches is also worth concern. The quality of internet sources can vary significantly, and incorporating such data without enough consideration m ay introduce noise or misleading information to the generated outputs. Future work will further explore a more stable and reliable method of retrieval augmentation.  Known challenges of LLMs include:  Presenting false information when it does not have the answer.  Presenting out -of-date or generic information when the user expects a specific, current response.  Creating a response from non -authoritative sources.  Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.  '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = read_pdf(file_path)\n",
    "text[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a9e0a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction to RAG  Retrieval -augmented generation (RAG) is an AI framework for improving the quality of  LLM -generated  responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM -based question answering system has two main benefits: It ensures that the model has access to the  most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.  Prior research has introduced retrieval techniques to incorporate relevant knowledge and augment  generation, as  exemplified  by retrieval  augmented  generation ( RAG). In this framework, the input to  models is augmented  by prepending  relevant  documents  that are retrieved  from  an external  knowledge  corpus .  While  RAG  serves  as a practicable complement to  LLMs, its effectiveness is contingent upon the relevance  and accuracy  of the retrieved  documents . The heavy  reliance  of generation',\n",
       " 'documents . The heavy  reliance  of generation  on the retrieved  knowledge r aises significant concerns about the  model’s  behaviour  and performance in scenarios  where  retrieval  may  fail or return  inaccurate  results.  A low-quality  retriever  is prone  to introducing a substantial amount of irrelevant information  impeding the  models from  acquiring accurate knowledge  and potentially  misleading  them,  resulting  in issues  such  as hallucinations.  However,  most conventional  RAG  approaches  indiscriminately  incorporate  the retrieved documents,  regardless  of whether  these  documents are  relevant  or not. Furthermore, current  methods  mostly  treat  complete  documents as  reference  knowledge  both  during  retrieval  and utilization . But a considerable  portion  of the text within these retrieved  documents is often  nonessential for  generation,  which  should  not have been  equally  referred  to and involved  in RAG.  Introduction to CRAG  Although retrieval -augmented',\n",
       " 'to CRAG  Although retrieval -augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retri eval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different k nowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large -scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose -then recompo se algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them.  Limitations  While we primarily proposed to improve the RAG framework from a corrective perspective, how to detect and correct the wrong knowledge more accurately and effectively still requires further study. Although CRAG can be seamlessly coupled with various RAG',\n",
       " 'CRAG can be seamlessly coupled with various RAG -based approaches, fine -tuning a retrieval evaluator is inevitable. In addition, potential bias introduced by web searches is also worth concern. The quality of internet sources can vary significantly, and incorporating such data without enough consideration m ay introduce noise or misleading information to the generated outputs. Future work will further explore a more stable and reliable method of retrieval augmentation.  Known challenges of LLMs include:  Presenting false information when it does not have the answer.  Presenting out -of-date or generic information when the user expects a specific, current response.  Creating a response from non -authoritative sources.  Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunks = pdf_to_chunks(text, 220, 10)\n",
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94c44d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'e7842eaf-e492-4026-95cf-0c98b2ed5ccf-0',\n",
       "  'text': 'Introduction to RAG  Retrieval -augmented generation (RAG) is an AI framework for improving the quality of  LLM -generated  responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM -based question answering system has two main benefits: It ensures that the model has access to the  most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.  Prior research has introduced retrieval techniques to incorporate relevant knowledge and augment  generation, as  exemplified  by retrieval  augmented  generation ( RAG). In this framework, the input to  models is augmented  by prepending  relevant  documents  that are retrieved  from  an external  knowledge  corpus .  While  RAG  serves  as a practicable complement to  LLMs, its effectiveness is contingent upon the relevance  and accuracy  of the retrieved  documents . The heavy  reliance  of generation'},\n",
       " {'id': 'e7842eaf-e492-4026-95cf-0c98b2ed5ccf-1',\n",
       "  'text': 'documents . The heavy  reliance  of generation  on the retrieved  knowledge r aises significant concerns about the  model’s  behaviour  and performance in scenarios  where  retrieval  may  fail or return  inaccurate  results.  A low-quality  retriever  is prone  to introducing a substantial amount of irrelevant information  impeding the  models from  acquiring accurate knowledge  and potentially  misleading  them,  resulting  in issues  such  as hallucinations.  However,  most conventional  RAG  approaches  indiscriminately  incorporate  the retrieved documents,  regardless  of whether  these  documents are  relevant  or not. Furthermore, current  methods  mostly  treat  complete  documents as  reference  knowledge  both  during  retrieval  and utilization . But a considerable  portion  of the text within these retrieved  documents is often  nonessential for  generation,  which  should  not have been  equally  referred  to and involved  in RAG.  Introduction to CRAG  Although retrieval -augmented'},\n",
       " {'id': 'e7842eaf-e492-4026-95cf-0c98b2ed5ccf-2',\n",
       "  'text': 'to CRAG  Although retrieval -augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retri eval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different k nowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large -scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose -then recompo se algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them.  Limitations  While we primarily proposed to improve the RAG framework from a corrective perspective, how to detect and correct the wrong knowledge more accurately and effectively still requires further study. Although CRAG can be seamlessly coupled with various RAG'},\n",
       " {'id': 'e7842eaf-e492-4026-95cf-0c98b2ed5ccf-3',\n",
       "  'text': 'CRAG can be seamlessly coupled with various RAG -based approaches, fine -tuning a retrieval evaluator is inevitable. In addition, potential bias introduced by web searches is also worth concern. The quality of internet sources can vary significantly, and incorporating such data without enough consideration m ay introduce noise or misleading information to the generated outputs. Future work will further explore a more stable and reliable method of retrieval augmentation.  Known challenges of LLMs include:  Presenting false information when it does not have the answer.  Presenting out -of-date or generic information when the user expects a specific, current response.  Creating a response from non -authoritative sources.  Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid = str(uuid.uuid4())\n",
    "uid\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        'id' : f'{uid}-{chunk_idx}',\n",
    "        'text' : chunk\n",
    "    } for chunk_idx, chunk in enumerate(pdf_chunks)\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c189b61",
   "metadata": {},
   "source": [
    "# Text Embedding and Storing Vector in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68295c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Pinecone' has no attribute 'from_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#openai.api_key = OPENAI_API_KEY\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#model = 'text-embedding-ada-002'\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     return embeddings_list\u001b[39;00m\n\u001b[0;32m     27\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[1;32m---> 28\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m \u001b[43mPinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m(pdf_chunks, embeddings, index_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Pinecone' has no attribute 'from_documents'"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "    \n",
    "index_name = 'chatbot'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(name=index_name,\n",
    "                    dimension=1536,\n",
    "                    metric='cosine',\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud='aws', \n",
    "                        region='us-west-2'\n",
    "                    )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "#openai.api_key = OPENAI_API_KEY\n",
    "#model = 'text-embedding-ada-002'\n",
    "\n",
    "# def create_embeddings(pdf_chunks):\n",
    "    \n",
    "#     embeddings_list = []\n",
    "#     for chunk in pdf_chunks:\n",
    "#         res = openai.Embedding.create(input=[chunk], engine=model)\n",
    "#         embeddings_list.append(res['data'][0]['embedding'])\n",
    "    \n",
    "#     return embeddings_list\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "docsearch = Pinecone.from_documents(pdf_chunks, embeddings, index_name)\n",
    "\n",
    "\n",
    "# pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# if index_name not in pc.list_indexes():\n",
    "    \n",
    "#     pc.create_index(index_name=index_name, \n",
    "#                     dimension=1536, \n",
    "#                     metric='cosine', \n",
    "#                     spec=ServerlessSpec(\n",
    "#                         cloud='aws', \n",
    "#                         region='us-west-2'\n",
    "#                     )\n",
    "#     )\n",
    "\n",
    "# index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "446e8e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embddings \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m embddings\n",
      "Cell \u001b[1;32mIn[22], line 22\u001b[0m, in \u001b[0;36mcreate_embeddings\u001b[1;34m(pdf_chunks, model)\u001b[0m\n\u001b[0;32m     20\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pdf_chunks:\n\u001b[1;32m---> 22\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     embeddings_list\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_list\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "embddings = create_embeddings(pdf_chunks, model)\n",
    "embddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c651a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Pinecone' has no attribute 'from_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[43membedding_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPINECONE_API_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m vectors\n",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m, in \u001b[0;36membedding_vectorstore\u001b[1;34m(pdf_chunks, PINECONE_API_KEY, OPENAI_API_KEY)\u001b[0m\n\u001b[0;32m     21\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[0;32m     22\u001b[0m index_name \u001b[38;5;241m=\u001b[39m init_pinecone(PINECONE_API_KEY)\n\u001b[1;32m---> 24\u001b[0m vectorestore \u001b[38;5;241m=\u001b[39m \u001b[43mPinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m(pdf_chunks, embedding\u001b[38;5;241m=\u001b[39membeddings, index_name\u001b[38;5;241m=\u001b[39mindex_name)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vectorestore\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Pinecone' has no attribute 'from_documents'"
     ]
    }
   ],
   "source": [
    "vectors = embedding_vectorstore(pdf_chunks, PINECONE_API_KEY, OPENAI_API_KEY)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c265c4f",
   "metadata": {},
   "source": [
    "# Store embeddings in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c467ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid vector value passed: cannot interpret type <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m embedding_list \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     18\u001b[0m chunk_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43membedding_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\pinecone\\utils\\error_handling.py:10\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ProtocolError):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\pinecone\\data\\index.py:171\u001b[0m, in \u001b[0;36mIndex.upsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_req is not supported when batch_size is provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upsert in parallel, please follow: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.pinecone.io/docs/insert-data#sending-upserts-in-parallel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsert_batch(vectors, namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size must be a positive integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\pinecone\\data\\index.py:194\u001b[0m, in \u001b[0;36mIndex._upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespace)])\n\u001b[0;32m    190\u001b[0m vec_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    193\u001b[0m     UpsertRequest(\n\u001b[1;32m--> 194\u001b[0m         vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvec_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict,\n\u001b[0;32m    196\u001b[0m         _check_type\u001b[38;5;241m=\u001b[39m_check_type,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    198\u001b[0m     ),\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    200\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\pinecone\\data\\index.py:190\u001b[0m, in \u001b[0;36mIndex._upsert_batch.<locals>.<lambda>\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upsert_batch\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m, vectors: Union[List[Vector], List[\u001b[38;5;28mtuple\u001b[39m], List[\u001b[38;5;28mdict\u001b[39m]], namespace: Optional[\u001b[38;5;28mstr\u001b[39m], _check_type: \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m UpsertResponse:\n\u001b[0;32m    189\u001b[0m     args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespace)])\n\u001b[1;32m--> 190\u001b[0m     vec_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: \u001b[43mVectorFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_check_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    193\u001b[0m         UpsertRequest(\n\u001b[0;32m    194\u001b[0m             vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(vec_builder, vectors)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    200\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\pinecone\\data\\vector_factory.py:60\u001b[0m, in \u001b[0;36mVectorFactory.build\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_dict_to_vector(item, check_type)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid vector value passed: cannot interpret type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid vector value passed: cannot interpret type <class 'list'>"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = 'chatbot'\n",
    "# pc.create_index(index_name, \n",
    "#                 dimension=768,\n",
    "#                 metric='cosine',\n",
    "#                 spec=ServerlessSpec(\n",
    "#                     cloud=\"aws\",\n",
    "#                     region=\"us-west-2\"\n",
    "#                 )\n",
    "# )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "for idx, embedding in enumerate(embeddings):\n",
    "    embedding_list = embedding.tolist()\n",
    "    \n",
    "    chunk_id = f'chunk_{idx+1}'\n",
    "    index.upsert([embedding_list], [chunk_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc687a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69be8eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 768,\n",
       "              'host': 'chatbot-wlh2nlv.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chatbot',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2032be32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20151d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
