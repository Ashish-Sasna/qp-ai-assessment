{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc19852e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b474baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import openai\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pinecone\n",
    "import tiktoken\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from Credentials import OPENAI_API_KEY, PINECONE_API_KEY\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone as lang_pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c764df",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d6ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "curr_dir = (os.getcwd()).replace('\\\\', '/')\n",
    "file_path = curr_dir + '/Dataset/Corrective RAG.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d425de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('p50k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97dfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(document):\n",
    "       \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pdf_text = loader.load()\n",
    "    pdf_text[0].page_content = pdf_text[0].page_content.replace('\\n', '')\n",
    "\n",
    "    return pdf_text    \n",
    "        \n",
    "def pdf_to_chunks(text, \n",
    "                  chunk_size, \n",
    "                  overlap):\n",
    "    \n",
    "    split_text = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "                                                chunk_overlap=overlap, \n",
    "                                                length_function=tiktoken_len)\n",
    "    docs = split_text.split_text(text[0].page_content)\n",
    "    \n",
    "    chunks = [str(doc) for doc in docs]\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b753a09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = read_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79e8222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Introduction to RAG  Retrieval -augmented generation (RAG) is an AI framework for improving the quality of  LLM -generated  responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM -based question answering system has two main benefits: It ensures that the model has access to the  most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.  Prior research has introduced retrieval techniques to incorporate relevant knowledge and augment  generation, as  exemplified  by retrieval  augmented  generation ( RAG). In this framework, the input to  models is augmented  by prepending  relevant  documents  that are retrieved  from  an external  knowledge  corpus .  While  RAG  serves  as a practicable complement to  LLMs, its effectiveness is contingent upon the relevance  and accuracy  of the retrieved  documents . The heavy  reliance  of generation  on the retrieved  knowledge r aises significant concerns about the  model’s  behaviour  and performance in scenarios  where  retrieval  may  fail or return  inaccurate  results.  A low-quality  retriever  is prone  to introducing a substantial amount of irrelevant information  impeding the  models from  acquiring accurate knowledge  and potentially  misleading  them,  resulting  in issues  such  as hallucinations.  However,  most conventional  RAG  approaches  indiscriminately  incorporate  the retrieved documents,  regardless  of whether  these  documents are  relevant  or not. Furthermore, current  methods  mostly  treat  complete  documents as  reference  knowledge  both  during  retrieval  and utilization . But a considerable  portion  of the text within these retrieved  documents is often  nonessential for  generation,  which  should  not have been  equally  referred  to and involved  in RAG.  Introduction to CRAG  Although retrieval -augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retri eval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different k nowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large -scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose -then recompo se algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them.  Limitations  While we primarily proposed to improve the RAG framework from a corrective perspective, how to detect and correct the wrong knowledge more accurately and effectively still requires further study. Although CRAG can be seamlessly coupled with various RAG -based approaches, fine -tuning a retrieval evaluator is inevitable. In addition, potential bias introduced by web searches is also worth concern. The quality of internet sources can vary significantly, and incorporating such data without enough consideration m ay introduce noise or misleading information to the generated outputs. Future work will further explore a more stable and reliable method of retrieval augmentation.  Known challenges of LLMs include:  Presenting false information when it does not have the answer.  Presenting out -of-date or generic information when the user expects a specific, current response.  Creating a response from non -authoritative sources.  Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.  ', metadata={'source': 'D:/GitUpload/qp-ai-assessment/Dataset/Corrective RAG.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9e0a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_chunks = pdf_to_chunks(text, 220, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea4a3b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = tiktoken_len(text[0].page_content)\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94c44d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '07ff60c0-87ee-49d6-aabe-681ed759cd7f-0',\n",
       "  'text': 'Introduction to RAG  Retrieval -augmented generation (RAG) is an AI framework for improving the quality of  LLM -generated  responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM -based question answering system has two main benefits: It ensures that the model has access to the  most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.  Prior research has introduced retrieval techniques to incorporate relevant knowledge and augment  generation, as  exemplified  by retrieval  augmented  generation ( RAG). In this framework, the input to  models is augmented  by prepending  relevant  documents  that are retrieved  from  an external  knowledge  corpus .  While  RAG  serves  as a practicable complement to  LLMs, its effectiveness is contingent upon the relevance  and accuracy  of the retrieved  documents . The heavy  reliance  of generation'},\n",
       " {'id': '07ff60c0-87ee-49d6-aabe-681ed759cd7f-1',\n",
       "  'text': 'documents . The heavy  reliance  of generation  on the retrieved  knowledge r aises significant concerns about the  model’s  behaviour  and performance in scenarios  where  retrieval  may  fail or return  inaccurate  results.  A low-quality  retriever  is prone  to introducing a substantial amount of irrelevant information  impeding the  models from  acquiring accurate knowledge  and potentially  misleading  them,  resulting  in issues  such  as hallucinations.  However,  most conventional  RAG  approaches  indiscriminately  incorporate  the retrieved documents,  regardless  of whether  these  documents are  relevant  or not. Furthermore, current  methods  mostly  treat  complete  documents as  reference  knowledge  both  during  retrieval  and utilization . But a considerable  portion  of the text within these retrieved  documents is often  nonessential for  generation,  which  should  not have been  equally  referred  to and involved  in RAG.  Introduction to CRAG  Although retrieval -augmented'},\n",
       " {'id': '07ff60c0-87ee-49d6-aabe-681ed759cd7f-2',\n",
       "  'text': 'to CRAG  Although retrieval -augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retri eval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different k nowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large -scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose -then recompo se algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them.  Limitations  While we primarily proposed to improve the RAG framework from a corrective perspective, how to detect and correct the wrong knowledge more accurately and effectively still requires further study. Although CRAG can be seamlessly coupled with various RAG'},\n",
       " {'id': '07ff60c0-87ee-49d6-aabe-681ed759cd7f-3',\n",
       "  'text': 'CRAG can be seamlessly coupled with various RAG -based approaches, fine -tuning a retrieval evaluator is inevitable. In addition, potential bias introduced by web searches is also worth concern. The quality of internet sources can vary significantly, and incorporating such data without enough consideration m ay introduce noise or misleading information to the generated outputs. Future work will further explore a more stable and reliable method of retrieval augmentation.  Known challenges of LLMs include:  Presenting false information when it does not have the answer.  Presenting out -of-date or generic information when the user expects a specific, current response.  Creating a response from non -authoritative sources.  Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid = str(uuid.uuid4())\n",
    "uid\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        'id' : f'{uid}-{chunk_idx}',\n",
    "        'text' : chunk\n",
    "    } for chunk_idx, chunk in enumerate(pdf_chunks)\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c189b61",
   "metadata": {},
   "source": [
    "# Text Embedding and Indexing in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68295c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7f2ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4}},\n",
       " 'total_vector_count': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "    \n",
    "index_name = 'chatbot'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(name=index_name,\n",
    "                    dimension=1536,\n",
    "                    metric='cosine',\n",
    "                    spec=ServerlessSpec(\n",
    "                        cloud='aws', \n",
    "                        region='us-west-2'\n",
    "                    )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e97f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'id')\n",
      "(1, 'text')\n"
     ]
    }
   ],
   "source": [
    "batch_limit = 100\n",
    "\n",
    "texts = []\n",
    "\n",
    "for text in enumerate(data[0].):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dce6e36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 4}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid = str(uuid.uuid4())\n",
    "chunk_id = []\n",
    "embeddings = []\n",
    "chunk_id = [str(f'{uid}-{idx}') for idx in range(len(pdf_chunks))]\n",
    "embeddings = embed.embed_documents(pdf_chunks)\n",
    "metadata = [{'chunk': chunk} for chunk in pdf_chunks]\n",
    "index.upsert(vectors=zip(chunk_id, \n",
    "                         embeddings, \n",
    "                         metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9c93efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4}},\n",
       " 'total_vector_count': 4}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c265c4f",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20151d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "text_field = 'chunk'\n",
    "\n",
    "vectorstore = lang_pinecone(index, \n",
    "                            embed.embed_query, \n",
    "                            text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa30311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Introduction to RAG  Retrieval -augmented generation (RAG) is an AI framework for improving the quality of  LLM -generated  responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM -based question answering system has two main benefits: It ensures that the model has access to the  most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.  Prior research has introduced retrieval techniques to incorporate relevant knowledge and augment  generation, as  exemplified  by retrieval  augmented  generation ( RAG). In this framework, the input to  models is augmented  by prepending  relevant  documents  that are retrieved  from  an external  knowledge  corpus .  While  RAG  serves  as a practicable complement to  LLMs, its effectiveness is contingent upon the relevance  and accuracy  of the retrieved  documents . The heavy  reliance  of generation'),\n",
       " Document(page_content='to CRAG  Although retrieval -augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retri eval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different k nowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return suboptimal documents, large -scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose -then recompo se algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them.  Limitations  While we primarily proposed to improve the RAG framework from a corrective perspective, how to detect and correct the wrong knowledge more accurately and effectively still requires further study. Although CRAG can be seamlessly coupled with various RAG'),\n",
       " Document(page_content='CRAG can be seamlessly coupled with various RAG -based approaches, fine -tuning a retrieval evaluator is inevitable. In addition, potential bias introduced by web searches is also worth concern. The quality of internet sources can vary significantly, and incorporating such data without enough consideration m ay introduce noise or misleading information to the generated outputs. Future work will further explore a more stable and reliable method of retrieval augmentation.  Known challenges of LLMs include:  Presenting false information when it does not have the answer.  Presenting out -of-date or generic information when the user expects a specific, current response.  Creating a response from non -authoritative sources.  Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What is RAG?'\n",
    "\n",
    "# Retrieveing the top 3 chunks with possible answer using similarity search\n",
    "vectorstore.similarity_search(query, \n",
    "                              k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2110e8",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c00a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                 model='gpt-3.5-turbo', \n",
    "                 temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"If you don't know the answer, just say you don't know the answer. \n",
    "Don't try to make up an answer.\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=['context', \n",
    "                                         'query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f5936fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(k=3)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                 chain_type='stuff', \n",
    "                                 retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11af331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you don't know the answer, just say you don't know the answer. \n",
      "Don't try to make up an answer.\n",
      "tags=['Pinecone'] vectorstore=<langchain_community.vectorstores.pinecone.Pinecone object at 0x0000020DBD71DAC0>\n",
      "\n",
      "Question: What is RAG?\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(query=query, context=retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a957a230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\DG\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"RAG stands for Retrieval-augmented generation. It is an AI framework that enhances the quality of responses generated by language models (LLMs) by incorporating external sources of knowledge to supplement the model's internal information representation. RAG ensures access to current, reliable facts and allows users to verify the model's sources for accuracy and trustworthiness.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a32601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0dbfe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 932 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"RAG stands for Retrieval-augmented generation. It is an AI framework that enhances the quality of responses generated by language models by incorporating external sources of knowledge to supplement the model's internal information representation. RAG ensures access to current, reliable facts and allows users to verify the model's claims for accuracy.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(qa, \n",
    "             query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d5204e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = 'What is a tire?'\n",
    "qa.run(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b0b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 875 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(qa, query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed9561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
